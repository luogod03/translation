{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è‹±æ–‡è¯„è®ºæ‰¹é‡ç¿»è¯‘ï¼ˆå¸¦æ–­ç‚¹ç»­ä¼  & GPU åŠ é€Ÿï¼‰\n",
    "æœ¬ Notebook ä½¿ç”¨ Helsinkiâ€‘NLP/opusâ€‘mtâ€‘enâ€‘zh æ¨¡å‹ï¼Œå°† CSV ä¸­çš„è‹±æ–‡è¯„è®ºç¿»è¯‘ä¸ºä¸­æ–‡ã€‚\n",
    "\n",
    "- âœ… æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œæ¯æ‰¹ç¿»è¯‘åå³æ—¶ä¿å­˜ã€‚\n",
    "- âœ… æ”¯æŒ GPU åŠ é€Ÿï¼Œç¿»è¯‘é€Ÿåº¦æ›´å¿«ã€‚\n",
    "- âœ… ç¿»è¯‘å®Œæˆåè‡ªåŠ¨ä¸‹è½½ `translated_output.csv`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# å®‰è£…å¿…è¦ä¾èµ–\n",
    "!pip install -q langdetect transformers sentencepiece pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from langdetect import detect, LangDetectException\n",
    "from google.colab import files\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# è®¾ç½®è®¾å¤‡ & åŠ è½½æ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡ï¼š\", device)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ä¸Šä¼ ä½ çš„ CSV æ–‡ä»¶ï¼ˆå¿…é¡»åŒ…å« `text` åˆ—ï¼‰\n",
    "uploaded = files.upload()\n",
    "input_file = list(uploaded.keys())[0]\n",
    "print(\"âœ… ä¸Šä¼ æ–‡ä»¶ï¼š\", input_file)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# è¯»å– CSVï¼ˆè‡ªåŠ¨å¤„ç†ç¼–ç  & å¼‚å¸¸è¡Œï¼‰\n",
    "try:\n",
    "    df = pd.read_csv(input_file, encoding='utf-8', on_bad_lines='skip')\n",
    "except Exception:\n",
    "    df = pd.read_csv(input_file, encoding='gbk', on_bad_lines='skip')\n",
    "print(f\"ğŸ“„ æ€»å…± {len(df)} æ¡è¯„è®ºå¾…å¤„ç†ï¼Œå‰5æ¡é¢„è§ˆï¼š\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# æ£€æŸ¥æ˜¯å¦å·²æœ‰éƒ¨åˆ†ç¿»è¯‘æ–‡ä»¶\n",
    "partial_file = 'translated_partial.csv'\n",
    "if os.path.exists(partial_file):\n",
    "    df_partial = pd.read_csv(partial_file, encoding='utf-8')\n",
    "    done_indices = set(df_partial.index)\n",
    "    df.loc[done_indices, 'text'] = df_partial.loc[done_indices, 'text']\n",
    "    print(f\"ğŸ”„ å·²åŠ è½½ {len(done_indices)} æ¡å·²ç¿»è¯‘å†…å®¹ï¼Œç»§ç»­ä»ä¸­æ–­ä½ç½®å¤„ç†ã€‚\")\n",
    "else:\n",
    "    done_indices = set()\n",
    "    print(\"ğŸ”„ æ— ä¸­æ–­ç»­ä¼ è®°å½•ï¼Œä»å¤´å¼€å§‹ç¿»è¯‘ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# å®šä¹‰è¯­è¨€æ£€æµ‹å’Œç¿»è¯‘å‡½æ•°\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def translate_batch(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# æ‰¹é‡ç¿»è¯‘ + æ–­ç‚¹ç»­ä¼ ä¿å­˜\n",
    "batch_size = 16\n",
    "total = len(df)\n",
    "for i in tqdm(range(0, total, batch_size), desc=\"ç¿»è¯‘è¿›åº¦\"):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_idx = batch_df.index.tolist()\n",
    "    if all(idx in done_indices for idx in batch_idx):\n",
    "        continue\n",
    "    texts = batch_df['text'].fillna('').astype(str).tolist()\n",
    "    mask = [is_english(t) for t in texts]\n",
    "    if any(mask):\n",
    "        to_translate = [t for t, m in zip(texts, mask) if m]\n",
    "        try:\n",
    "            translated = translate_batch(to_translate)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ æ‰¹æ¬¡å‘ç”Ÿé”™è¯¯ï¼Œè·³è¿‡ï¼š{e}\")\n",
    "            translated = to_translate\n",
    "        it = iter(translated)\n",
    "        new_texts = [next(it) if m else t for t, m in zip(texts, mask)]\n",
    "    else:\n",
    "        new_texts = texts\n",
    "    df.loc[batch_idx, 'text'] = new_texts\n",
    "    done_indices.update(batch_idx)\n",
    "    df.loc[list(batch_idx), ['text']].to_csv(partial_file, index=list(batch_idx), header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ç¿»è¯‘å®Œæˆï¼Œå¯¼å‡ºæœ€ç»ˆæ–‡ä»¶å¹¶ä¸‹è½½\n",
    "output_file = 'translated_output.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "files.download(output_file)\n",
    "print(f\"âœ… ç¿»è¯‘å®Œæˆï¼Œå·²ç”Ÿæˆæ–‡ä»¶ï¼š{output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.9" }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
